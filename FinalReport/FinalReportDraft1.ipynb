{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e31b7153511f0485",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Dropped Reads Analysis Pipeline\n",
    "\n",
    "Alex Jorjorian \n",
    "\n",
    "# Introduction \n",
    "\n",
    "The Dropped Reads Analysis Pipeline (DRAP) is an open-source tool I developed to address a critical issue frequently arising in our work at Clear Labs. As a company that develops and markets an end-to-end sequencing assay automation platform, we often utilize short-read sequencers for our assays, including the Sars-Cov2 wastewater surveillance and microbial isolate profiling assays. These assays are each paired with a bioinformatics pipeline that processes the sequencing output to answer biological questions. However, when processing bioinformatics pipelines, remove reads from consideration due to a multitude of problems with the reads; we call these lost reads “dropped reads.” While quality control is essential, a high drop rate of reads belies underlying issues with the quality and cost-effectiveness of our assays.   \n",
    "\n",
    "In these bioinformatics pipelines, filtering steps are applied so that spurious reads, be they low quality or contaminants, do not confound the results or as a byproduct of mapping reads to a reference genome before further analysis.  Reads are also lost at sample demultiplexing. Losing reads to filtering, demultiplexing, or failed mapping corresponds directly to lost revenue and lost assay power, as every read thrown out represents a spot on a flowcell that is not providing helpful information. A 50% dropped reads rate is a 50% reduction in the number of samples that could have been sequenced to a given quality, all things being equal. A decrease in the number of samples allowed per assay leads to a greater cost per sample, as each run has fixed costs. This increase in cost provides substantial motivation to understand why an assay is losing reads.   \n",
    "\n",
    "DRAP is designed to be user-friendly, addressing the problem of dropped reads by characterizing user-defined tranches of dropped reads across all samples in a sequencing run. It is a workflow definition language (WDL) pipeline that takes an input zip file and returns an interactive HTML report containing various metrics for each group of dropped reads. This user-friendly format allows scientists without bioinformatics training to easily explore relationships between dropped reads metrics. By studying these reports, scientists can understand the mechanisms by which reads are lost and implement changes to mitigate those losses. For example, if many reads are lost at demultiplexing, there may be an issue with dimerization of amplification primers, which could be filtered out with a more aggressive SPRI treatment. DRAP can also provide rapid customer support by allowing for rapid characterization of low-yield runs. We hope DRAP will be a valuable and accessible tool for institutions or individuals working with short-read sequencing data. \n",
    "\n",
    "# Background\n",
    "\n",
    "Short-read sequencers, such as the various platforms offered by Illumina, generate millions of DNA sequence reads sampled from a solution of DNA fragments known as a library (1). These reads are typically paired such that for each fragment, 150-300bp of the sequence is covered from either end of the fragment with an unsequenced insert in between in the case of fragments longer than the sum of the read lengths. A sequencing assay consists of a series of biochemical manipulations on biological samples such that they produce fragments that can answer a given biological question. For example, a COVID-19 surveillance assay would make a series of fragments from the SARs-Cov2 genome, which would then be sequenced to create a set of reads. A bioinformatics pipeline would then process these reads by mapping the COVID reference, and the variation of consensus alignment could be leveraged to ascertain what strain a sample contained.   \n",
    "\n",
    "In any bioinformatics pipeline, there are multiple stages where reads may be dropped. We will focus on an archetypal bioinformatics workflow employed at Clear Labs for this proof of concept. The first and often most significant source of dropped reads is demultiplexing, using a sample index on each read to sort the reads into individual samples; this is because to be demultiplexed, the read must rather closely match the index barcode and aberrant reads often do not match (2). Reads that make it into sample fastqs are then usually filtered to meet minimum read length and quality standards using tools such as fastp or trimmomatic. This filtered reads group can be analyzed to understand why they have low-quality scores or short lengths. Finally, reads can be dropped due to failing alignment to some form of reference, be that local alignment like BLAST or global alignment like BWA. This may be due to low quality or the reads coming from an unexpected source.   \n",
    "\n",
    "DRAP attempts to characterize the primary sources of dropped reads: contamination or off-target amplification, dimerization, low read signal quality, and fragment size. Contamination is a ubiquitous problem in sequencing biological samples, as an utterly sterile environment is impossible outside a clean room. This problem is so widespread that many reference genomes have false variations due to contaminated reads(3). Reads can drop due to contamination and can show up at either demultiplexing if the contaminant DNA fails to ligate to sample index adapters or at an alignment level when they fail to align to the expected reference.   \n",
    "\n",
    "Dimerization is the process of two sequencing adapters or PCR primers in targeted assays ligating to each other without a sample fragment between them. Many library preparation procedures use size selection to filter out dimers that will inevitably form. However, if even a small amount of dimers enter a PCR reaction, they can outcompete genuine fragments and saturate the flowcell at loading (4, 5). The dimers are dropped at demultiplexing, size filtering, or alignment, depending on pipeline settings.   \n",
    "\n",
    "Basecall quality score-based read filtering uses Q-scores output by the sequencing instrument to determine if a read is trustworthy enough to pass filtering based on user-defined thresholds. How q-scores are calculated varies by the sequencing platform, considering signal-to-noise ratio, signal intensity, phasing information, and more. Filtering software trims off poor-quality bases and will filter entire reads if their trimmed length or average quality is too low. The underlying causes of low-quality reads are numerous. Some level of quality filtering is inevitable and hard to mitigate as some molecular clusters that become reads simply won’t have enough copies to provide a strong signal. But, some quality failure modalities can be detected by analyzing the reads, the spatial position, and fragment sizes. Bubbles on the flowcell can cause read clusters to miss sequencing cycles, leading to low qscores for those cycles. This would appear as dropped reads spatially colocated in bubble or streak-shaped patterns (1). Overloading can lead to low SNR and show up as low-quality reads next to one another as their light bleeds into one another. These flowcell issues can be ameliorated by altering flowcell loading procedures. Finally, large fragment sizes can lead to low-quality scores as they do not amplify on the flowcell as efficiently and will, therefore, have a lower signal and lower quality; this can be detected by loosely mapping the dropped reads to the target reference and analyzing insert size (6). \n",
    "\n",
    "# Design\n",
    "\n",
    "![Pipeline Diagram](Methods/DRAP.png)\n",
    "Figure 1: A flow diagram of the various tasks in the DRAP workflow. \n",
    "\n",
    "The DRAP is designed to efficiently, scalable, and flexiblely detect and quantify reads dropped by all the mechanisms discussed thus far with maximum portability. To this end, DRAP is written in the Workflow Definition Language (WDL). WDL is an open-source scripting language that several engines can run, most prominently Cromwell and, in the case of this project, miniWDL (7). A WDL workflow comprises multiple task scripts strung together by designating the inputs of one task as the outputs from another. Each task consists of input/output variables, a command section consisting of bash code, and a runtime section defining a docker image that the task will be run in a container based upon. These docker images are webhosted, allowing the WDL tasks to run identically on all systems. WDL is highly scalable as it runs tasks as parallel as possible, given the CPU and memory resources allocated to each task and user hardware. WDL allows anyone to download and run DRAP on their data with little effort. \n",
    "\n",
    "The DRAP runs in 3 sequential stages: input processing via consolidate\\_samples.wdl, read group processing, and output consolidation. As input, DRAP takes a .json file that defines a list of strings to group fastqs on, the run name and paths to the reference fasta, the Kraken database,  a list of adapters to search for, and a zip file containing all the fastqs and reference fastas to process. The consolidation task then concatenates all fastqs containing the exact group string on a strand basis, yielding an R1 and R2 fastqs file for each group. This is done as the reasons reads are dropped should not vary by sample. The consolidation task also concatenates all fastas in the zip file, and this is then sent to a reference index-building task so that the fasta containing all target sequences can be used in the bwa task. \n",
    "\n",
    "After the samples are consolidated, the files for each dropped read group are sent to analysis tasks in parallel. \n",
    "\n",
    "![Pipeline Diagram](Methods/Kraken.png)\n",
    "\n",
    "Figure 2: Example multiqc Kraken Metrics\n",
    "\n",
    "The Kraken task runs kraken2 with a user-defined database on each group of reads to detect what organisms may contaminate the reads. Kraken2 is a Kmer-based taxonomy calling software that efficiently finds the best fit taxonomic assignment for each read based on a reference database, in our case, a reasonably extensive refseq PlusPF-16 database (8). If a significant portion of reads dropped at any step are from an unexpected organism, scientist can rework their assay to mitigate the risk of contamination. \n",
    "\n",
    "![Pipeline Diagram](Methods/Fastqc.png)\n",
    "\n",
    "Figure 3: Example multiqc fastqc metrics\n",
    "\n",
    "The fastqc task runs the fastq quality analysis tool separately on each R1 and R2 fastq file. This provides many read quality metrics, from quality score by cycle to duplicate sequence counts. By aggregating the fastqc metrics for each dropped reads group, DRAP gives end users a high degree of flexibility to investigate many questions about why read quality is poor. \n",
    "\n",
    "![Pipeline Diagram](Methods/Flowcell.png)\n",
    "\n",
    "Figure 4: Example of flowcell metrics plots and tables\n",
    "\n",
    "The flowcell metrics task consists of custom-written Python code that leverages the read id string for each read in the fastq files to analyze the positions of each dropped read. These positions are processed to produce a plot of the positions of each read that can be used to visually inspect if there are any spatial patterns to dropped read positions indicative of bubbles or flowcell defects. These positions are also processed with the clustering algorithm Scikit Learn dbscan to detect and output any clustered read positions, which may indicate bubbles or aggregates (9). The clusters are also plotted as an image, and a metric file tracking the cluster sizes and counts is returned. An accessory task in the results consolidation stage processes the outputs from each instantiation of flowcell metrics into a format compatible with multiqc.  \n",
    "\n",
    "![Pipeline Diagram](Methods/Dimers.png)\n",
    "\n",
    "Figure 5: Example of dimer metrics output \n",
    "\n",
    "The dimer metrics task runs bbduk from the bbmap suite of bioinformatic tools and processes the outputs into a convenient format. BBduk is a tool used to detect and trim adapter sequences from input reads, but for the DRAP, it is used to detect the percentage of input reads that contain adapter or primer sequences defined by the user (10). This provides one method for the user to estimate the prevalence of primer or adapter dimers in each dropped read group. This does require the user to know the primer and adapter sequences used in the assay. The results for all read groups are combined into a multiqc compatible file in a separate consolidation task. \n",
    "\n",
    "![Pipeline Diagram](Methods/Mapping.png)\n",
    "\n",
    "Figure 6: Examples of loose mapping metrics\n",
    "\n",
    "For alignment analysis DRAP employs a subworkflow, bwa loose mapping metrics. As the name implies, the task runs the BWA mem alignment algorithm (11)  with highly permissive settings to maximize mapping, allowing DRAP to capture as much information as possible about potentially low-quality dropped reads. BWA is an industry-standard global alignment algorithm that outputs sam files of read alignments. The loose mapping workflow then passes these alignments to the bamStat task that processes the alignments with samtools stats (12) to provide general mapping metrics and Picard tools collectInserSizeMetrics to calculate the insert size distributions for each read group (13). The mapping metrics allow the user to understand how many reads in each dropped read group are mapped to one of the target reference sequences, giving a secondary indication of contamination. Samtools also returns a series of other secondary metrics that could indicate the quality of the reads and their composition. As discussed earlier, the insert size metrics allow the user to inspect the insert size distribution for excessively long inserts that may lead to lower read quality or very short fragment sizes indicative of dimerization. \n",
    "\n",
    "![Pipeline Diagram](Methods/Stats.png)\n",
    "Figure 7: Example of the multiqc summary table\n",
    "\n",
    "Once the analysis and analysis-specific consolidation steps are run, their output files are passed on to the multiqc task. This task contains a simple bash script that runs multiqc on all the output files, consolidating them into a single interactive html report. Multiqc is a powerful bioinformatics report aggregation tool that can search a directory for output files from over 140 different bioinformatics tools and consolidate them into a single portable HTML report (14). Multiqc also allows users to leverage its custom file processing utilities to add custom sections to the report using specially structured file formats. DRAP does this for the flowcell metrics and dimer metrics reports. Multiqc QC reports also allow users to create new visualizations, such as scatterplots, extending their utility from the data they contain. The final output of each DRAP run is a single html file using the user-defined report name as the title. \n",
    "\n",
    "The DRAP code's fine details are beyond this report's scope, but all files are publicly available in the linked GitHub repository (add link). All tasks in the DRAP use custom-built Docker images built off open-source base images; these are uploaded to Docker Hub and are publicly available. The DRAP repo is broken up into subdirectories: Docker containing the Dockerfiles for each image along with code to build them; a resources directory containing default references and accessory files; a task directory containing each WDL task file; and a workflow directory containing the DRAP workflow. An additional Google Drive repository (linked here) includes the kraken2 database used in this project and all test data zip files. \n",
    "\n",
    "## Materials and Testing Methodology \n",
    "\n",
    "The DRAP pipeline was tested on data from 12 Clear Labs runs from two short read assays, microbial surveillance, and SARS-Cov2 wastewater. These runs were selected from runs our field service team deemed problematic, development runs we had known problems with, and runs that had dropped reads metrics, filtering rate, mapping coverage, and mapping rate far below expectations based on a database query. To extract the dropped reads, our existing bioinformatic pipelines were modified to save reads dropped at demux, each filtering step, and alignment steps to a zip file, along with any references used for alignment. The code changes to extract this data are outside this project's scope. All output reports and an index describing the run will be available on Google Drive. (add link) In the results section, I will describe a subset of these runs and provide examples of how the DRAP HTML reports provide utility in troubleshooting why reads are dropped in bioinformatics analysis of sequencing assays. \n",
    "\n",
    "# Results  \n",
    "\n",
    "The DRAP was run on the 12 datasets outlined above during development and testing. In this section, I will first discuss the DRAP's computational performance and then highlight the 3 case studies from the testing set. \n",
    "\n",
    "## Runtime Analysis\n",
    "\n",
    "![Runtime Analysis](Runtime/Runtimes.png)\n",
    "\n",
    "Figure 8: Regression plots of total runtime and total CPU seconds per MB of input data\n",
    "\n",
    "![Runtime Analysis](Runtime/Tasks.png)\n",
    "\n",
    "Figure 9: Boxplots of task runtime in CPU seconds. \n",
    "\n",
    "The DRAP must be maximally performant across various hardware platforms to maximize its utility and ease of adoption. To investigate this, I have plotted the total clock time and CPU time utilized by the DRAP across the 12 test runs against the total data processed in Mb. All test runs were run locally on a 16-thread 32GB memory Linux machine. As shown in Figure 8, the CPU runtime is only 11.8 seconds per MB, while the total runtime is slower at 16.5 seconds per MB. This indicates bottlenecks in the pipeline on this hardware where tasks are idle while waiting for resources. This is likely in the Kraken task as it is allocated 32 GB of memory, meaning only one instance can be run simultaneously. We also see outlier runs that take far longer than average for a given dataset size, meaning some tasks must be hitting a slowdown. Figure 9 shows that the task with the highest CPU runtimes in BWA alignment, with some outlier instances taking several CPU hours to process, is likely due to many mappable reads in one of the read groups. The Flowcell Metrics task is typically quite fast; however, there are instances when it takes over a CPU hour. This is due to the dbscan clustering algorithm finding many candidate clusters in some runs. Future versions of DRAP may seek to implement optimizations to increase clustering speed. The generation code for this runtime analysis is available in the appendix and on Git Hub. \n",
    "\n",
    "## Case Study 1 \n",
    "\n",
    "![Multiqc Summary Table](Case1/Table.png)\n",
    "\n",
    "Figure 10: Multiqc Summary Table from case study 1\n",
    "\n",
    "![Base Composition](Case1/R1_bases.png)\n",
    "\n",
    "Figure 11: Read 1 bases are evenly balanced\n",
    "\n",
    "![Base Composition](Case1/R2_Bases.png)\n",
    "\n",
    "Figure 12: Read 2 bases are skewed towards AT\n",
    "\n",
    "![Quality Scores](Case1/QualityShift.png)\n",
    "\n",
    "Figure 13: Quality scores are lower in the second read of the undermined group\n",
    "\n",
    "Case study 1 involves a development run from our Microbial Isolate assay that we determined was contaminated with polyA sequences. In this run, all samples were listeria isolates. In the DRAP report, it is immediately apparent that there is a high degree of GC bias in the dropped read groups, see Figure 10. Investigating further in the fastq report section, the first strand is somewhat balanced in the base composition per cycle (Figure 11). Still, the second strand (Figure 12) is highly enriched in thymine basecalls, consistent with a poly A sequence ligated to the end of many fragments in the run. We also see a stark drop in average q-score in the second strand of undermined reads, and this is consistent with many in-phase poly A  clusters reducing the SNR and, consequently, the quality score of nearby clusters (Figure 13). If DRAP reports were available during development, this failure modality would have been detected more quickly, allow for more rapid troubleshooting and shorter development time. \n",
    "\n",
    "## Case Study 2 \n",
    "\n",
    "![Kraken Phylum Breakdown](Case2/Kraken.png)\n",
    "\n",
    "Figure 14: Phylum level taxonomic breakdown from case study 2. \n",
    "\n",
    "\n",
    "For the second case study, I selected a run that the customer flagged as contaminated to see if DRAP could allow for easier contamination detection. Upon inspecting the Kraken outputs of the DRAP, seen in figure n we see that 4 million read pairs from the unmapped reads group are assigned to various bacterial phylum.  The run is from our microbial isolate assay, in which each sample is derived from a microbial monoculture, a fact that is leveraged to select the reference genome to align to for each sample. For this run the reference genomes used were from species in the Pseudomonadota and Bacilliota phyla, however we also see Bacteroidota in the Kraken phylum breakdown. Given that the run is based on isolates the only mechanism to find 10% of reads from a different phylum is contamination. When this run was escalated to our customer support team, a bioinformatics scientist was pulled from other tasks to confirm the failure mode. With the DRAP, the customer support team could simply look at the HTML report and tell at a glance that contamination was present. If contamination exists it should typically show up in unmapped reads as they will fail to align to the expected reference.\n",
    "\n",
    "\n",
    "## Case Study 3\n",
    "\n",
    "![Kraken Report](Case3/Kraken.png)\n",
    "\n",
    "Figure 15: Kraken Report showing bacterial contamination \n",
    "\n",
    "![GC Bias](Case3/GC.png)\n",
    "\n",
    "Figure 16: GC Bias plot showing contamination\n",
    "\n",
    "![Insert Size](Case3/Inserts.png)\n",
    "\n",
    "For the final case study, I utilized the Clear Labs big query database to find a production Wastewater surveillance run with poor success metrics, low coverage of the Sars-Cov2 reference genome, low read mapping rate, and a high degree of filtering. This run is typical of the type of run a customer might escalate to our support team, making it a perfect use case for using DRAP reports to ascertain the root cause of poor performance. From the DRAP report, the largest contribution to poor performance is high levels of contamination, as seen in figure 15 approximately 2.5 million reads are assigned to bacteria in this Covid assay. This indicates that the method the customer uses to capture and extract SARS-Cov2 from wastewater is suboptimal and also captures many bacteria. The GC bias plot from fastqc also confirms contamination; unmapped reads in yellow show GC% centered around 60%, whereas undetermined reads in red show a bimodal distribution (figure 16). This is likely because COVID-19 has an even base balance, whereas the bacterial contaminants that fail mapping have high GC content. Contamination does not tell the whole story; however, according to Kraken, there are an additional 4.2 million unclassified reads. Another possible source of poor performance could be the dimerization of PCR primers; Figure 17 shows that many short insert sequences map to the COVID reference in reads that failed to map with strict settings. These may be primer dimers that are undetected by the dimer metrics task as the tiling primers for this assay were not supplied, as they would flag every read as a dimer. This prospect would need to be experimentally validated, but it highlights the need to improve the dimer detection task for future assays. Another possible source of the short inserts could simply be heavy chemical degradation of the sample DNA leading to short fragments, many of which become too short to properly map.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Discussion and Future Efforts\n",
    "\n",
    "As the case studies described above show, the Dropped Reads Analysis Pipeline (DRAP) could be a force multiplier in developing and troubleshooting short-read sequencing assays. The DRAP is a relatively lightweight pipeline that can be quickly integrated into existing WDL-based workflows and other workflows with a small amount of wrapper code. Of particular utility is the Kraken-based contamination module, which is highly useful in troubleshooting unknown issues. In this project, I have found a few shortcomings of the pipeline; for example, the flowcell metrics module has yet to show any utility and requires optimization to cluster read positions efficiently. It is still of interest to leave in the pipeline however, as I was unable to source test runs that contained flowcell or bubbles, this does not mean that once implemented, these will not be detected.  The dimer metrics were also limited in my case studies by the list of sequences supplied to it to search for dimers upon, while in the undermined read groups, it consistently found thousands of Illumina adapter dimers. Case study one shows that it needed to be adequately designed to detect primer dimers from amplicon tiling-based assays. Future development will overhaul this module to detect amplicon primer dimers without marking all read from the amplicons as dimers, as this is the reason I did not supply the task with the ArticV5 primer set used in the Wastewater surveillance product. Additional development efforts will seek to extend the DRAP to long-read sequencing assays, add additional tasks to profile reads dropped by the basecalling software, and general runtime optimizations. Despite these challenges, the DRAP is ready to be integrated into Clear Labs development pipelines and utilized in an Open-source manner. \n",
    "\n",
    "# Appendix \n",
    "\n",
    "### Source code:\n",
    "\n",
    "All DRAP source code is located at the attached [GitHub repo.](https://github.com/Jorjorian/DRAP/tree/current)   \n",
    "DRAP is open source and free to use with citation. \n",
    "\n",
    "### Test Run Data:\n",
    "\n",
    "All test run data is located in the attached public [Google Drive link](https://drive.google.com/drive/u/0/folders/17pESpQ0JAtwC_b952vI4igu852nv9_6N).   \n",
    "The case studies are in zip files named case Studies 1, 2, and 3, respectively.   \n",
    "The following code block will download all the test runs and a directory containing their respective multiqc reports into a folder named project data in your working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "771b32e2c7450233",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T05:16:27.052422Z",
     "start_time": "2024-08-05T05:16:09.840621Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'DRAP'...\n",
      "Retrieving folder contents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1FiUILR_uwV6QuK0ttYu7lKlz1nXY1gG3 .DS_Store\n",
      "Retrieving folder 1zUiQhkblAkULeOCyCZ8dmqOmEaU1zFbc Databases\n",
      "Processing file 1goFTv9XzCURZ2wUHEpbCTzAkjO4UFLB0 k2_pluspf_16gb_20240605.tar.gz\n",
      "Processing file 1v2IYyraGiyUTPC3qlV2kgeApV9BiK6e4 DRAP Pipeline Diagram - Page 1 (2).png\n",
      "Retrieving folder 1lffKOPRAUZe6j1pehMs0MmAI6ZT8F3tZ Reports\n",
      "Processing file 1jzAziNciKvDQul9JEO0lHb1LP1_KFWi5 5ec23219c5594ab7b28b8b2b97191ecf.html\n",
      "Processing file 1iqtRfxUvellDG7eUlj2CutDR7TO9HB1L 38d4087d.html\n",
      "Processing file 1LWCzMtAgJyO83r4gQhG3TJ_T5u55Wo4Y 46d5b3846510427d85314ecbaf72fced.html\n",
      "Processing file 1YBgdUeZisRXTNb0fnVxiaDSHEJCCA-j5 355d0f54fdb14c14a06545fa828ab233.html\n",
      "Processing file 10SBzMhbOwhGE9t3hs06Udn5OgaNbHdVh 3057a86440224ce5872f06626c933228.html\n",
      "Processing file 1SOAd1QZ9P0ZddBLCHrGJud3mv0i3cZFq 36026b19.html\n",
      "Processing file 1M5IsMgKa9vH_G0WkzssJmTpkbhmSWU59 460762a8f0044f25917c1674329c3b25.html\n",
      "Processing file 1jdf0B5ng8-onkVHCNxyzQ-cNrJqp771A ac37e2d7d431414898e9f2cb77d33d97.html\n",
      "Processing file 1dTxfAiaHhYP4G4g75Yw0yYn_Qxg_S4ck be55d03a8d7d4c2e9cec4385628ece0b.html\n",
      "Processing file 1lfEsFuYDaNxfKBrW9N9Ol1a1a62D7R4Y d43ba1895f704c6887ed802a7b3c4335.html\n",
      "Processing file 1n27UoOtfSW-t2KDrIqRSGjnvBacTTOM_ e43fdcc8788b4f55ab452074e123737b.html\n",
      "Processing file 1t6XHF8iqyy4Fp5VxNt0cXUUqZG-EkGc8 fa2dd5a95aa04735854eb35f8d55aa35.html\n",
      "Processing file 1T4JGLEJxkXaIDUBVeX7rBiPZfxfPIngM reports_5ec23219c5594ab7b28b8b2b97191ecf_BIP-Sv14-Drap_2024-07-29-00h_05m_12s_soterclone-4565_5ec23219c5594ab7b28b8b2b97191ecf_BIP-Sv14-Drap_2024-07-29-00h_05m_12s_soterclone-4565.zip\n",
      "Processing file 1f3smDAhIt1k9759L1jXw3Ocqt9i-93O_ reports_38d4087d_BIP-Sv14-Drap_2024-07-29-02h_30m_13s_soterclone-11264_38d4087d_BIP-Sv14-Drap_2024-07-29-02h_30m_13s_soterclone-11264.zip\n",
      "Processing file 15Me3ltDQNcNo-wenZ91MPzL3IkeglCj- reports_46d5b3846510427d85314ecbaf72fced_BIP-Sv14-Drap_2024-07-28-00h_10m_12s_soterclone-22060_46d5b3846510427d85314ecbaf72fced_BIP-Sv14-Drap_2024-07-28-00h_10m_12s_soterclone-22060.zip\n",
      "Processing file 1ySdikzz2XZNSkeksKs2RtyircFRpvvbY reports_3057a86440224ce5872f06626c933228_BIP-Sv14-Drap_2024-07-28-00h_10m_12s_soterclone-24916_3057a86440224ce5872f06626c933228_BIP-Sv14-Drap_2024-07-28-00h_10m_12s_soterclone-24916.zip\n",
      "Processing file 1IosXzeHnNTy24QbaLvZH4Wf7oZKscO16 reports_36026b19_BIP-Sv14-Drap_2024-07-29-02h_30m_13s_soterclone-20672_36026b19_BIP-Sv14-Drap_2024-07-29-02h_30m_13s_soterclone-20672.zip\n",
      "Processing file 1ZdWcQqSl5NUS-xdM-W9MXGWYjFosI3ie reports_460762a8f0044f25917c1674329c3b25_BIP-Xv1-filtered_outputs_2024-07-26-23h_30m_14s_soterclone-9194_field_outputs_460762a8f0044f25917c1674329c3b25_BIP-Xv1-filtered_outputs_2024-07-26-23h_30m_14s_soterclone-9194.zip\n",
      "Processing file 1Pk0Tvxv456vQqecmf6L6Qymek620D5EW reports_ac37e2d7d431414898e9f2cb77d33d97_BIP-Sv14-Drap_2024-07-29-02h_25m_12s_soterclone-5110_ac37e2d7d431414898e9f2cb77d33d97_BIP-Sv14-Drap_2024-07-29-02h_25m_12s_soterclone-5110.zip\n",
      "Processing file 1U4_4MxGDPYtkfefjgTOXo92oarBo6GF0 reports_be55d03a8d7d4c2e9cec4385628ece0b_BIP-Sv14-Drap_2024-07-29-02h_25m_12s_soterclone-20918_be55d03a8d7d4c2e9cec4385628ece0b_BIP-Sv14-Drap_2024-07-29-02h_25m_12s_soterclone-20918.zip\n",
      "Processing file 1a9YEGXwGEbgbUv0Ym9tfIZ4JnmkTE4uq reports_d43ba1895f704c6887ed802a7b3c4335_BIP-Sv14-Drap_2024-07-28-00h_10m_12s_soterclone-3333_d43ba1895f704c6887ed802a7b3c4335_BIP-Sv14-Drap_2024-07-28-00h_10m_12s_soterclone-3333.zip\n",
      "Processing file 113Uh8G2LWS4nfH7fvAQ6F6LjWq4M0jx7 reports_e43fdcc8788b4f55ab452074e123737b_BIP-Sv14-Drap_2024-07-28-20h_35m_14s_soterclone-20143_e43fdcc8788b4f55ab452074e123737b_BIP-Sv14-Drap_2024-07-28-20h_35m_14s_soterclone-20143.zip\n",
      "Retrieving folder 1L_LbENYc0x3WQyXAX-TnCVrGwc7VputW Run Data\n",
      "Processing file 1A12CLg9X4-NjjuKhwfh-rbYt6r6_Gll8 355d0f54fdb14c14a06545fa828ab233_BIP-Xv1-filtered_outputs_2024-07-26-22h_20m_14s_soterclone-29974.zip\n",
      "Processing file 1T4JGLEJxkXaIDUBVeX7rBiPZfxfPIngM reports_5ec23219c5594ab7b28b8b2b97191ecf_BIP-Sv14-Drap_2024-07-29-00h_05m_12s_soterclone-4565_5ec23219c5594ab7b28b8b2b97191ecf_BIP-Sv14-Drap_2024-07-29-00h_05m_12s_soterclone-4565.zip\n",
      "Processing file 1f3smDAhIt1k9759L1jXw3Ocqt9i-93O_ reports_38d4087d_BIP-Sv14-Drap_2024-07-29-02h_30m_13s_soterclone-11264_38d4087d_BIP-Sv14-Drap_2024-07-29-02h_30m_13s_soterclone-11264.zip\n",
      "Processing file 15Me3ltDQNcNo-wenZ91MPzL3IkeglCj- reports_46d5b3846510427d85314ecbaf72fced_BIP-Sv14-Drap_2024-07-28-00h_10m_12s_soterclone-22060_46d5b3846510427d85314ecbaf72fced_BIP-Sv14-Drap_2024-07-28-00h_10m_12s_soterclone-22060.zip\n",
      "Processing file 1ySdikzz2XZNSkeksKs2RtyircFRpvvbY reports_3057a86440224ce5872f06626c933228_BIP-Sv14-Drap_2024-07-28-00h_10m_12s_soterclone-24916_3057a86440224ce5872f06626c933228_BIP-Sv14-Drap_2024-07-28-00h_10m_12s_soterclone-24916.zip\n",
      "Processing file 1IosXzeHnNTy24QbaLvZH4Wf7oZKscO16 reports_36026b19_BIP-Sv14-Drap_2024-07-29-02h_30m_13s_soterclone-20672_36026b19_BIP-Sv14-Drap_2024-07-29-02h_30m_13s_soterclone-20672.zip\n",
      "Processing file 1ZdWcQqSl5NUS-xdM-W9MXGWYjFosI3ie reports_460762a8f0044f25917c1674329c3b25_BIP-Xv1-filtered_outputs_2024-07-26-23h_30m_14s_soterclone-9194_field_outputs_460762a8f0044f25917c1674329c3b25_BIP-Xv1-filtered_outputs_2024-07-26-23h_30m_14s_soterclone-9194.zip\n",
      "Processing file 1Pk0Tvxv456vQqecmf6L6Qymek620D5EW reports_ac37e2d7d431414898e9f2cb77d33d97_BIP-Sv14-Drap_2024-07-29-02h_25m_12s_soterclone-5110_ac37e2d7d431414898e9f2cb77d33d97_BIP-Sv14-Drap_2024-07-29-02h_25m_12s_soterclone-5110.zip\n",
      "Processing file 1U4_4MxGDPYtkfefjgTOXo92oarBo6GF0 reports_be55d03a8d7d4c2e9cec4385628ece0b_BIP-Sv14-Drap_2024-07-29-02h_25m_12s_soterclone-20918_be55d03a8d7d4c2e9cec4385628ece0b_BIP-Sv14-Drap_2024-07-29-02h_25m_12s_soterclone-20918.zip\n",
      "Processing file 1a9YEGXwGEbgbUv0Ym9tfIZ4JnmkTE4uq reports_d43ba1895f704c6887ed802a7b3c4335_BIP-Sv14-Drap_2024-07-28-00h_10m_12s_soterclone-3333_d43ba1895f704c6887ed802a7b3c4335_BIP-Sv14-Drap_2024-07-28-00h_10m_12s_soterclone-3333.zip\n",
      "Processing file 113Uh8G2LWS4nfH7fvAQ6F6LjWq4M0jx7 reports_e43fdcc8788b4f55ab452074e123737b_BIP-Sv14-Drap_2024-07-28-20h_35m_14s_soterclone-20143_e43fdcc8788b4f55ab452074e123737b_BIP-Sv14-Drap_2024-07-28-20h_35m_14s_soterclone-20143.zip\n",
      "Processing file 14PKUd43MHW1N7tcjdS9tJklW6Q5fKdbV reports_fa2dd5a95aa04735854eb35f8d55aa35_BIP-Xv1-filtered_outputs_2024-07-26-23h_30m_13s_soterclone-15087_field_outputs_fa2dd5a95aa04735854eb35f8d55aa35_BIP-Xv1-filtered_outputs_2024-07-26-23h_30m_13s_soterclone-15087.zip\n",
      "Processing file 1c2gPTdfih3AdrpLeSi6HVRgv8dhSHhb6 run_catalog.txt\n",
      "Processing file 12BFxY9_ad3GQR-DSSZFX3N_9PXz6q6tM runs_to_check.csv\n",
      "Processing file 1c2gPTdfih3AdrpLeSi6HVRgv8dhSHhb6 run_catalog.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1FiUILR_uwV6QuK0ttYu7lKlz1nXY1gG3\n",
      "To: /home/alex/DataspellProjects/DRAP/FinalReport/project_data/.DS_Store\n",
      "100%|██████████| 6.15k/6.15k [00:00<00:00, 10.0MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1goFTv9XzCURZ2wUHEpbCTzAkjO4UFLB0\n",
      "From (redirected): https://drive.google.com/uc?id=1goFTv9XzCURZ2wUHEpbCTzAkjO4UFLB0&confirm=t&uuid=2a5dc6ff-9c71-42a3-9c8c-80732083fa7b\n",
      "To: /home/alex/DataspellProjects/DRAP/FinalReport/project_data/Databases/k2_pluspf_16gb_20240605.tar.gz\n",
      "  5%|▌         | 657M/12.0G [00:06<01:52, 101MB/s]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m google_drive_folder_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m17pESpQ0JAtwC_b952vI4igu852nv9_6N\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      8\u001B[0m subprocess\u001B[38;5;241m.\u001B[39mrun([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgit\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclone\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-b\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcurrent\u001B[39m\u001B[38;5;124m\"\u001B[39m, github_repo_url, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDRAP\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m----> 9\u001B[0m \u001B[43mgdown\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_folder\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mid\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgoogle_drive_folder_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mproject_data\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquiet\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/NGS/lib/python3.8/site-packages/gdown/download_folder.py:325\u001B[0m, in \u001B[0;36mdownload_folder\u001B[0;34m(url, id, output, quiet, proxy, speed, use_cookies, remaining_ok, verify, user_agent, skip_download, resume)\u001B[0m\n\u001B[1;32m    322\u001B[0m     files\u001B[38;5;241m.\u001B[39mappend(local_path)\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m--> 325\u001B[0m local_path \u001B[38;5;241m=\u001B[39m \u001B[43mdownload\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    326\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhttps://drive.google.com/uc?id=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mid\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    327\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    328\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquiet\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquiet\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    329\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproxy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    330\u001B[0m \u001B[43m    \u001B[49m\u001B[43mspeed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspeed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    331\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cookies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cookies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    332\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverify\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverify\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    333\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    334\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    335\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m local_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    336\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m quiet:\n",
      "File \u001B[0;32m~/miniconda3/envs/NGS/lib/python3.8/site-packages/gdown/download.py:368\u001B[0m, in \u001B[0;36mdownload\u001B[0;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001B[0m\n\u001B[1;32m    366\u001B[0m     pbar \u001B[38;5;241m=\u001B[39m tqdm\u001B[38;5;241m.\u001B[39mtqdm(total\u001B[38;5;241m=\u001B[39mtotal, unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mB\u001B[39m\u001B[38;5;124m\"\u001B[39m, initial\u001B[38;5;241m=\u001B[39mstart_size, unit_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    367\u001B[0m t_start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m--> 368\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m res\u001B[38;5;241m.\u001B[39miter_content(chunk_size\u001B[38;5;241m=\u001B[39mCHUNK_SIZE):\n\u001B[1;32m    369\u001B[0m     f\u001B[38;5;241m.\u001B[39mwrite(chunk)\n\u001B[1;32m    370\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m quiet:\n",
      "File \u001B[0;32m~/miniconda3/envs/NGS/lib/python3.8/site-packages/requests/models.py:816\u001B[0m, in \u001B[0;36mResponse.iter_content.<locals>.generate\u001B[0;34m()\u001B[0m\n\u001B[1;32m    814\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    815\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 816\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw\u001B[38;5;241m.\u001B[39mstream(chunk_size, decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    817\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m ProtocolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    818\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ChunkedEncodingError(e)\n",
      "File \u001B[0;32m~/miniconda3/envs/NGS/lib/python3.8/site-packages/urllib3/response.py:934\u001B[0m, in \u001B[0;36mHTTPResponse.stream\u001B[0;34m(self, amt, decode_content)\u001B[0m\n\u001B[1;32m    932\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    933\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_fp_closed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 934\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    936\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m data:\n\u001B[1;32m    937\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m data\n",
      "File \u001B[0;32m~/miniconda3/envs/NGS/lib/python3.8/site-packages/urllib3/response.py:877\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt, decode_content, cache_content)\u001B[0m\n\u001B[1;32m    874\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m amt:\n\u001B[1;32m    875\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer\u001B[38;5;241m.\u001B[39mget(amt)\n\u001B[0;32m--> 877\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raw_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    879\u001B[0m flush_decoder \u001B[38;5;241m=\u001B[39m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m (amt \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data)\n\u001B[1;32m    881\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/miniconda3/envs/NGS/lib/python3.8/site-packages/urllib3/response.py:812\u001B[0m, in \u001B[0;36mHTTPResponse._raw_read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    809\u001B[0m fp_closed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    811\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_error_catcher():\n\u001B[0;32m--> 812\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fp_closed \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n\u001B[1;32m    814\u001B[0m         \u001B[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001B[39;00m\n\u001B[1;32m    815\u001B[0m         \u001B[38;5;66;03m# Close the connection when no data is returned\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    820\u001B[0m         \u001B[38;5;66;03m# not properly close the connection in all cases. There is\u001B[39;00m\n\u001B[1;32m    821\u001B[0m         \u001B[38;5;66;03m# no harm in redundantly calling close.\u001B[39;00m\n\u001B[1;32m    822\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/miniconda3/envs/NGS/lib/python3.8/site-packages/urllib3/response.py:789\u001B[0m, in \u001B[0;36mHTTPResponse._fp_read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    787\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    788\u001B[0m     chunk_amt \u001B[38;5;241m=\u001B[39m max_chunk_amt\n\u001B[0;32m--> 789\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk_amt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n\u001B[1;32m    791\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/NGS/lib/python3.8/http/client.py:459\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    456\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    457\u001B[0m     \u001B[38;5;66;03m# Amount is given, implement using readinto\u001B[39;00m\n\u001B[1;32m    458\u001B[0m     b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mbytearray\u001B[39m(amt)\n\u001B[0;32m--> 459\u001B[0m     n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadinto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmemoryview\u001B[39m(b)[:n]\u001B[38;5;241m.\u001B[39mtobytes()\n\u001B[1;32m    461\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    462\u001B[0m     \u001B[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001B[39;00m\n\u001B[1;32m    463\u001B[0m     \u001B[38;5;66;03m# and self.chunked\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/NGS/lib/python3.8/http/client.py:503\u001B[0m, in \u001B[0;36mHTTPResponse.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    498\u001B[0m         b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmemoryview\u001B[39m(b)[\u001B[38;5;241m0\u001B[39m:\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength]\n\u001B[1;32m    500\u001B[0m \u001B[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001B[39;00m\n\u001B[1;32m    501\u001B[0m \u001B[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001B[39;00m\n\u001B[1;32m    502\u001B[0m \u001B[38;5;66;03m# (for example, reading in 1k chunks)\u001B[39;00m\n\u001B[0;32m--> 503\u001B[0m n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadinto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m n \u001B[38;5;129;01mand\u001B[39;00m b:\n\u001B[1;32m    505\u001B[0m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[1;32m    506\u001B[0m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[1;32m    507\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_conn()\n",
      "File \u001B[0;32m~/miniconda3/envs/NGS/lib/python3.8/socket.py:669\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    667\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    668\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 669\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    670\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    671\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/NGS/lib/python3.8/ssl.py:1274\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m   1270\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1271\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1272\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1273\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1274\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1275\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[0;32m~/miniconda3/envs/NGS/lib/python3.8/ssl.py:1132\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1130\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1131\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1132\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1133\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1134\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import subprocess\n",
    "import gdown\n",
    "\n",
    "github_repo_url = \"https://github.com/Jorjorian/DRAP.git\"\n",
    "google_drive_folder_id = \"17pESpQ0JAtwC_b952vI4igu852nv9_6N\"\n",
    "\n",
    "subprocess.run([\"git\", \"clone\", \"-b\", \"current\", github_repo_url, 'DRAP'])\n",
    "gdown.download_folder(id=google_drive_folder_id, output='project_data', quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82ebcf5031bf9a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "\n",
    "### Drap Pipeline Installation and Testing \n",
    "\n",
    "The code block below will attempt to install the DRAP dependencies onto your active environment. This code only works on linux machines and there are many other mechanism by which this may fail. Therefore, I will also outline the steps to install the dependencies in the enviroment from which you are running this jupyter notebook.   \n",
    "\n",
    "1. First make sure you have python 3.7 or above  \n",
    "1. Next install docker as described [here](https://www.docker.com/get-started/)   \n",
    "1. Install miniwdl and docker filing using the procedure outlined [here](https://github.com/chanzuckerberg/miniwdl?tab=readme-ov-file\\#install-miniwdl).   \n",
    "1. Next clone the DRAP GitHub repo and checkout the branch current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f024f198eabc0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import subprocess\n",
    "\n",
    "def run_sudo_command(command, password):\n",
    "    command = f'echo {password} | sudo -S {command}'\n",
    "    process = subprocess.Popen(\n",
    "        command,\n",
    "        shell=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    output, error = process.communicate()\n",
    "    return output, error\n",
    "\n",
    "def install_docker(password):\n",
    "    os_type = os.popen('uname').read().strip()\n",
    "    print(f'Operating System: {os_type}')\n",
    "    if os_type == \"Linux\":\n",
    "        # Ensure you are using the correct Ubuntu release name (e.g., focal, jammy)\n",
    "        ubuntu_release = os.popen('lsb_release -cs').read().strip()\n",
    "\n",
    "        # Add the Docker GPG key to the appropriate location\n",
    "        gpg_command = (\n",
    "            \"curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\"\n",
    "        )\n",
    "        output, error = run_sudo_command(gpg_command, password)\n",
    "        if error:\n",
    "            print(f\"Error running command '{gpg_command}': {error.decode()}\")\n",
    "        else:\n",
    "            print(output.decode())\n",
    "\n",
    "        # Add Docker repository\n",
    "        repo_command = (\n",
    "            f'echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu {ubuntu_release} stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null'\n",
    "        )\n",
    "        output, error = run_sudo_command(repo_command, password)\n",
    "        if error:\n",
    "            print(f\"Error running command '{repo_command}': {error.decode()}\")\n",
    "        else:\n",
    "            print(output.decode())\n",
    "\n",
    "        commands = [\n",
    "            \"apt-get update\",\n",
    "            \"apt-get install -y apt-transport-https ca-certificates curl software-properties-common\",\n",
    "            \"apt-get install -y docker-ce\",\n",
    "            \"systemctl status docker\",\n",
    "            \"docker --version\"\n",
    "        ]\n",
    "\n",
    "        for command in commands:\n",
    "            output, error = run_sudo_command(command, password)\n",
    "            if error:\n",
    "                print(f\"Error running command '{command}': {error.decode()}\")\n",
    "            else:\n",
    "                print(output.decode())\n",
    "    elif os_type == \"Darwin\":\n",
    "        print(\"Please install Docker Desktop from https://www.docker.com/products/docker-desktop\")\n",
    "    elif os_type == \"Windows\":\n",
    "        print(\"Please install Docker Desktop from https://www.docker.com/products/docker-desktop\")\n",
    "    else:\n",
    "        print(f\"Unsupported OS type: {os_type}. Please install Docker manually.\")\n",
    "\n",
    "# Prompt for the sudo password\n",
    "password = getpass.getpass(prompt=\"Enter your sudo password: \")\n",
    "\n",
    "# Check if Docker is already installed, if not, install it\n",
    "docker_installed = os.system(\"docker --version\")\n",
    "\n",
    "if docker_installed != 0:\n",
    "    install_docker(password)\n",
    "else:\n",
    "    print(\"Docker is already installed.\")\n",
    "\n",
    "# Install miniwdl using pip\n",
    "!pip install miniwdl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1580072f16e61c44",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "The code block below should run DRAP on the 3 case study datasets, however should that fail one can run DRAP with the command outline below. \n",
    "\n",
    "miniwdl run path\\_to\\_drap\\_repo/workflows/DroppedReadsAnalysis.wdl \\-i path\\_to\\_project\\_data/CaseN.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51662375d5b9fae",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run DRAP on the case study datasets\n",
    "import os\n",
    "import subprocess\n",
    "subprocess.run([\"miniwdl\", \"run\", \"DRAP/workflows/DroppedReadsAnalysis.wdl\", \"-i\", \"case1.json\"])\n",
    "subprocess.run([\"miniwdl\", \"run\", \"DRAP/workflows/DroppedReadsAnalysis.wdl\", \"-i\", \"case2.json\"])\n",
    "subprocess.run([\"miniwdl\", \"run\", \"DRAP/workflows/DroppedReadsAnalysis.wdl\", \"-i\", \"case3.json\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## runtime analysis code\n",
    "The code block below will generate the runtime analysis plots."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ae683d236843f6e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import linregress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ZIP_DIR = 'project_data/RunData/'\n",
    "RUNS_DIR = '.'\n",
    "\n",
    "# Output CSV file\n",
    "OUTPUT_CSV = \"workflow_summary.csv\"\n",
    "\n",
    "task_cpu_allocation = {\n",
    "    \"bwaMem\": 8,\n",
    "    \"bamStat\": 1,\n",
    "    \"flowcell_metrics\": 1,\n",
    "    \"dimer_metrics\": 1,\n",
    "    \"combine_dimer_metrics\": 1,\n",
    "    \"runKraken\": 14,\n",
    "    \"consolidate_flowcell_metrics\": 1,\n",
    "    \"combine_images\": 1,\n",
    "    \"consolidate_samples\": 1,\n",
    "    \"fastqc\": 3,\n",
    "    \"multiqc\": 1,\n",
    "    \"fastaIndex\": 1,\n",
    "    # Add more tasks and their CPU allocations as needed\n",
    "}\n",
    "\n",
    "# Regex patterns for parsing the workflow.log\n",
    "start_pattern = re.compile(r'(\\d+-\\d+-\\d+ \\d+:\\d+:\\d+\\.\\d+) .*? ready :: job: \"(.*?)\"')\n",
    "finish_pattern = re.compile(r'(\\d+-\\d+-\\d+ \\d+:\\d+:\\d+\\.\\d+) .*? finish :: job: \"(.*?)\"')\n",
    "datetime_pattern = re.compile(r'(\\d+-\\d+-\\d+ \\d+:\\d+:\\d+\\.\\d+)')\n",
    "\n",
    "# Function to parse datetime from the log\n",
    "def parse_datetime(dt_str):\n",
    "    return datetime.strptime(dt_str, '%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "# Collect data for CSV\n",
    "data = []\n",
    "task_durations = []\n",
    "\n",
    "# Iterate over each output directory\n",
    "for item in os.listdir(RUNS_DIR):\n",
    "    item_path = os.path.join(RUNS_DIR, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        workflow_log_path = os.path.join(item_path, '_LAST', \"workflow.log\")\n",
    "        if os.path.isfile(workflow_log_path):\n",
    "            print(f\"Processing {item_path}\")\n",
    "\n",
    "            # Extract the base name of the directory as the run_name\n",
    "            run_name = item\n",
    "\n",
    "            # Get the corresponding zip file size\n",
    "            zip_file_path = os.path.join(ZIP_DIR, f\"{run_name}.zip\")\n",
    "            if os.path.isfile(zip_file_path):\n",
    "                zip_file_size = os.path.getsize(zip_file_path)\n",
    "            else:\n",
    "                zip_file_size = 0\n",
    "\n",
    "            # Initialize dictionaries to track start and end times\n",
    "            start_times = {}\n",
    "            end_times = {}\n",
    "            timestamps = []\n",
    "\n",
    "            # Parse the workflow.log to extract task runtimes\n",
    "            with open(workflow_log_path, 'r') as log_file:\n",
    "                for line in log_file:\n",
    "                    start_match = start_pattern.search(line)\n",
    "                    if start_match:\n",
    "                        dt_str, task_name = start_match.groups()\n",
    "                        start_times[task_name] = parse_datetime(dt_str)\n",
    "\n",
    "                    finish_match = finish_pattern.search(line)\n",
    "                    if finish_match:\n",
    "                        dt_str, task_name = finish_match.groups()\n",
    "                        end_times[task_name] = parse_datetime(dt_str)\n",
    "                    datetime_match = datetime_pattern.search(line)\n",
    "                    if datetime_match:\n",
    "                        dt_str = datetime_match.group(1)\n",
    "                        timestamps.append(parse_datetime(dt_str))\n",
    "\n",
    "            # Calculate total runtime for the workflow\n",
    "            total_cpu_seconds = 0\n",
    "            for task_name in start_times:\n",
    "                if task_name in end_times:\n",
    "                    runtime_seconds = (end_times[task_name] - start_times[task_name]).total_seconds()\n",
    "                    task_name = task_name.split(\"-\")[1]\n",
    "\n",
    "                    # Get CPU allocation for the task\n",
    "                    cpus = task_cpu_allocation.get(task_name, 1)  # Default to 1 if not specified\n",
    "                    cpu_seconds = runtime_seconds * cpus\n",
    "                    total_cpu_seconds += cpu_seconds\n",
    "\n",
    "                    # Store task durations for additional analysis\n",
    "                    task_durations.append((run_name, task_name, runtime_seconds, cpu_seconds))\n",
    "\n",
    "            if timestamps:\n",
    "                start_time = min(timestamps)\n",
    "                end_time = max(timestamps)\n",
    "                total_runtime_seconds = (end_time - start_time).total_seconds()\n",
    "            else:\n",
    "                total_runtime_seconds = 0\n",
    "            # Append the information to the data list\n",
    "            data.append([run_name, zip_file_size, total_runtime_seconds, total_cpu_seconds])\n",
    "\n",
    "        else:\n",
    "            print(f\"No workflow.log found in {item_path}\")\n",
    "\n",
    "# Write data to CSV\n",
    "with open(OUTPUT_CSV, mode='w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow([\"run_name\", \"zip_file_size\", \"total_runtime_seconds\", \"total_cpu_seconds\"])\n",
    "    csv_writer.writerows(data)\n",
    "\n",
    "print(f\"Summary written to {OUTPUT_CSV}\")\n",
    "\n",
    "# Load the data from CSV\n",
    "df = pd.read_csv(OUTPUT_CSV)\n",
    "df['zip_file_size_kb'] = df['zip_file_size'] / 1024 / 1024  # Convert bytes to MB\n",
    "\n",
    "# Initialize the plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for total CPU seconds\n",
    "sns.regplot(x='zip_file_size_kb', y='total_cpu_seconds', data=df, ax=axes[0])\n",
    "axes[0].set_xlabel('Zip File Size (MB)')\n",
    "axes[0].set_ylabel('Total CPU Runtime (cpu seconds)')\n",
    "axes[0].set_title('Total CPU Runtime vs. Zip File Size')\n",
    "slope_cpu, intercept_cpu, r_value_cpu, p_value_cpu, std_err_cpu = linregress(df['zip_file_size_kb'], df['total_cpu_seconds'])\n",
    "axes[0].text(0.05, 0.95, f'Slope: {slope_cpu:.2f} S/MB', transform=axes[0].transAxes, fontsize=12, verticalalignment='top')\n",
    "\n",
    "# Plot for total runtime seconds\n",
    "sns.regplot(x='zip_file_size_kb', y='total_runtime_seconds', data=df, ax=axes[1])\n",
    "axes[1].set_xlabel('Zip File Size (MB)')\n",
    "axes[1].set_ylabel('Total Runtime (seconds)')\n",
    "axes[1].set_title('Total Runtime vs. Zip File Size')\n",
    "slope_runtime, intercept_runtime, r_value_runtime, p_value_runtime, std_err_runtime = linregress(df['zip_file_size_kb'], df['total_runtime_seconds'])\n",
    "axes[1].text(0.05, 0.95, f'Slope: {slope_runtime:.2f} S/MB', transform=axes[1].transAxes, fontsize=12, verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('runtime_vs_zip_size_subplot.png')\n",
    "\n",
    "\n",
    "# Analyze task durations\n",
    "task_df = pd.DataFrame(task_durations, columns=[\"run_name\", \"task_name\", \"runtime_seconds\", \"cpu_seconds\"])\n",
    "\n",
    "\n",
    "# Plot CPU seconds per task\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(y='task_name', x='cpu_seconds', data=task_df)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Task Name')\n",
    "plt.xlabel('CPU Time (seconds)')\n",
    "plt.title('Distribution of CPU Time per Task')\n",
    "plt.tight_layout()\n",
    "plt.savefig('task_cpu_time_distribution.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "874f507e479c3dc0"
  },
  {
   "cell_type": "markdown",
   "id": "1cf9712fd9b39a57",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# References \n",
    "\n",
    "1. Ravi, R.K., Walton, K., Khosroheidari, M. (2018). MiSeq: A Next Generation Sequencing Platform for Genomic Analysis. In: DiStefano, J. (eds) Disease Gene Identification. Methods in Molecular Biology, vol 1706\\. Humana Press, New York, NY. https://doi.org/10.1007/978-1-4939-7471-9\\_12   \n",
    "1. Hu, T., Chitnis, N., Monos, D., & Dinh, A. (2021). Next-generation sequencing technologies: An overview. Human Immunology, 82(11), 801-811. [https://doi.org/10.1016/j.humimm.2021.02.012](https://doi.org/10.1016/j.humimm.2021.02.012)  \n",
    "1. Goig, G.A., Blanco, S., Garcia-Basteiro, A.L. et al. Contaminant DNA in bacterial sequencing experiments is a major source of false genetic variability. BMC Biol 18, 24 (2020). [https://doi.org/10.1186/s12915-020-0748-z](https://doi.org/10.1186/s12915-020-0748-z)  \n",
    "1. Shore S, Henderson JM, Lebedev A, Salcedo MP, Zon G, McCaffrey AP, et al. (2016) Small RNA Library Preparation Method for Next-Generation Sequencing Using Chemical Modifications to Prevent Adapter Dimer Formation. PLoS ONE 11(11): e0167009. [https://doi.org/10.1371/journal.pone.0167009](https://doi.org/10.1371/journal.pone.0167009)  \n",
    "1. Garafutdinov, R. R., Galimova, A. A., & Sakhabutdinova, A. R. (2020). The influence of quality of primers on the formation of primer dimers in PCR. Nucleotides, Nucleotides & Nucleic Acids, 39(9), 1251–1269. [https://doi.org/10.1080/15257770.2020.1803354](https://doi.org/10.1080/15257770.2020.1803354)  \n",
    "1. Tan, G., Opitz, L., Schlapbach, R., & Rehrauer, H. (2019). Long fragments achieve lower base quality in Illumina paired-end sequencing. Scientific Reports, 9(1), 1-7. [https://doi.org/10.1038/s41598-019-39076-7](https://doi.org/10.1038/s41598-019-39076-7)  \n",
    "1. Voss K, Van der Auwera G and Gentry J. Full-stack genomics pipelining with GATK4 \\+ WDL \\+ Cromwell \\[version 1; not peer reviewed\\]. F1000Research 2017, 6(ISCB Comm J):1381 (slides) DOI:10.7490/f1000research.1114634.1  \n",
    "1. Wood, D.E., Lu, J. & Langmead, B. Improved metagenomic analysis with Kraken 2\\. *Genome Biol* 20, 257 (2019). https://doi.org/10.1186/s13059-019-1891-0  \n",
    "1. Scikit-learn: Machine Learning in Python, Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, Édouard Duchesnay; 12(85):2825−2830, 2011\\.  \n",
    "1. Bushnell, Brian. BBMap: A Fast, Accurate, Splice-Aware Aligner. United States: N. p., 2014\\. Web.  \n",
    "1. Li, H. \"Aligning sequence reads, clone sequences and assembly contigs with BWA‐MEM.\" arXiv (2013).  \n",
    "1. Heng Li, Bob Handsaker, Alec Wysoker, Tim Fennell, Jue Ruan, Nils Homer, Gabor Marth, Goncalo Abecasis, Richard Durbin, 1000 Genome Project Data Processing Subgroup, The Sequence Alignment/Map format and SAMtools, Bioinformatics, Volume 25, Issue 16, August 2009, Pages 2078–2079, [https://doi.org/10.1093/bioinformatics/btp352](https://doi.org/10.1093/bioinformatics/btp352)  \n",
    "1. “Picard Toolkit.” 2019\\. Broad Institute, GitHub Repository. https://broadinstitute.github.io/picard/; Broad Institute  \n",
    "1. Philip Ewels, Måns Magnusson, Sverker Lundin, Max Käller, MultiQC: summarize analysis results for multiple tools and samples in a single report, Bioinformatics, Volume 32, Issue 19, October 2016, Pages 3047–3048, https://doi.org/10.1093/bioinformatics/btw354"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
